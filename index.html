<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>John Seon Keun Yi</title>
  
  <meta name="author" content="John Seon Keun Yi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>		
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>John Seon Keun Yi</name>
              </p>
              <p>
                I am a Computer Science Ph.D student at <a href="https://www.bu.edu/cs/">Boston University</a>.
		I am fortunate to be advised by Prof. <a href="https://www.leedokyun.com/">Dokyun "DK" Lee</a> and <a href="https://ai.bu.edu/ksaenko.html">Kate Saenko</a>.
              </p>
              <p>
                <!-- I work on computer vision and its use in robotics. -->
                I received Masters and Bachelors degrees in Computer Science from <a href="https://www.cc.gatech.edu/">Georgia Institute of Technology</a>, where I worked on data-efficient learning and methodologies for human-robot interaction.
              </p>

              <p style="text-align:center">
                <a href="mailto:johnsk9595@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1udQRLXpFSVz9dAC0vALyhudbUZHsM1fb/view?usp=share_link">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.tw/citations?user=LtqmnjEAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/johnsk95">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JohnYi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/JohnYi-circle2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interest mainly focuses on multi-agentic systems. Particularly, I am interested in identifying and mitigating bias and misinformation in foundation models. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/placeholder.png" alt="A Survey on Masked A" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>A Survey on Masked Autoencoder for Visual Self-supervised Learning.</papertitle>
              </a>
              <br>
              
              <br>
              <em></em>, 2023
              <br>
              
              <p></p>
              <p></p>
            </td>
          </tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/global_piqa_evaluating_physical.png" alt="paper" width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2510.24081v1">
      <papertitle>Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</papertitle>
    </a>
    <br>
    <strong>Baber Abbasi, Stella Biderman, David Stap, Duygu Ataman, Fabian Schmidt, Hila Gonen, Jiayi Wang, David Ifeoluwa Adelani, Venue: 2025</strong>
    <br>
    <em>Description: Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures presents an evaluation benchmark for large language models (LLMs) that covers a large number of languages and cultures, including many low-resource languages. The benchmark consists of over 50% examples referencing local foods, customs, traditions, or other culturally-specific elements, highlighting the importance of everyday knowledge in understanding language</em>
    <br>
    <a href="https://arxiv.org/pdf/2510.24081v1">paper</a> / <a href="#">project page</a> / <a href="#">code</a>
    <p></p>
    <p>This paper presents an evaluation benchmark for large language models (LLMs) that covers a large number of languages and cultures, including many low-resource languages. The benchmark consists of over 50% examples referencing local foods, customs, traditions, or other culturally-specific elements, highlighting the importance of everyday knowledge in understanding language</p>
  </td>
</tr>
<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/global_piqa_evaluating_physical.png" alt="global_piqa" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2510.24081">
                <papertitle>Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</papertitle>
              </a>
              <br>
              Various Authors
              <br>
              <em>arXiv</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2510.24081">paper</a>
              <p></p>
              <p>
An evaluation benchmark for physical commonsense reasoning covering 100+ languages and cultures.
              </p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infdriver.png" alt="8803_head" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://infdriver.github.io/">
                <papertitle>Uncertainty-Guided Never-Ending Learning to Drive</papertitle>
              </a>
              <br>
	      <a href="https://leilai125.github.io/">Lei Lai</a>, 
              <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>, 
              <a href="https://research.redhat.com/blog/project_member/sanjay-arora/">Sanjay Arora</a>,
              <strong>John Seon Keun Yi</strong>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://infdriver.github.io/">project page</a> /
              <a href="https://github.com/h2xlab/InfDriver">code</a> / 
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_Uncertainty-Guided_Never-Ending_Learning_to_Drive_CVPR_2024_paper.pdf">paper</a>
              <p></p>
              <p>
Continuously learning to drive from an infinite flow of YouTube driving videos.
              </p>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mae_flow.jpg" alt="mae_flow" width="160" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://www.ijcai.org/proceedings/2023/0762.pdf">
                <papertitle>A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond</papertitle>
              </a>
              <br>
              <a href="https://chaoningzhang.github.io/">Chaoning Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=HbqjLHYAAAAJ">Chenshuang Zhang,</a>, 
              <a href="https://junha1125.github.io/about/">Junha Song</a>, 
              <strong>John Seon Keun Yi</strong>,
              <a href="https://scholar.google.com/citations?user=XA8EOlEAAAAJ">In So Kweon</a>
              <br>
              <em>IJCAI</em>, 2023  
              <br>
							<a href="https://arxiv.org/pdf/2208.00173">arXiv</a>
              <p></p>
              <p>A comprehensive survey on Masked Autoencoders(MAE) in vision and other fields.</p>
            </td>
          </tr>
		
          <tr onmouseout="pt4al_stop()" onmouseover="pt4al_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pt4al_image'>
                  <img src='images/pt4al_after.jpeg' width="160"></div>
                <img src='images/pt4al_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function pt4al_start() {
                  document.getElementById('pt4al_image').style.opacity = "1";
                }

                function pt4al_stop() {
                  document.getElementById('pt4al_image').style.opacity = "0";
                }
                pt4al_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://seominseok0429.github.io/PT4AL/">
                <papertitle>PT4AL: Using Self-Supervised Pretext Tasks for Active Learning</papertitle>
              </a>
              <br>
              <strong>John Seon Keun Yi*</strong>,
              <a href="https://sites.google.com/view/minseokcv/">Minseok Seo*</a>, 
              <a href="https://sites.google.com/view/jongchanpark">Jongchan Park</a>, 
              <a href="https://sites.google.com/site/dgchoicv/">Dong-Geol Choi</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://seominseok0429.github.io/PT4AL/">project page</a> /
              <a href="https://www.youtube.com/watch?v=SN7U13MMbOk">video</a> /
              <a href="https://github.com/johnsk95/PT4AL">code</a> / 
              <a href="https://arxiv.org/abs/2201.07459">arXiv</a>
              <p></p>
              <p>
We use simple self-supervised pretext tasks and a loss-based sampler to sample both representative and difficult data from an unlabeled pool.
              </p>
            </td>
          </tr>		


			    <tr onmouseout="scene_stop()" onmouseover="scene_start()">
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='scene_image'>
                  <img src='images/scene_before.png' height="170" width="160"></div>
                <img src='images/scene_after.png' height="170" width="160">
              </div>
			        <script type="text/javascript">
			          function scene_start() {
			            document.getElementById('scene_image').style.opacity = "1";
			          }

			          function scene_stop() {
			            document.getElementById('scene_image').style.opacity = "0";
			          }
			          scene_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			          <a href="https://arxiv.org/pdf/2201.01901.pdf">
			            <papertitle>Incremental Object Grounding Using Scene Graphs</papertitle>
			          </a>
			          <br>
			          <strong>John Seon Keun Yi*</strong>,
			          <a href="https://github.com/ywkim0606">Yoonwoo Kim*</a>,
			          <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                <br>
                <em>arXiv</em>, 2021  
                <br>
			          <a href="https://arxiv.org/pdf/2201.01901">arXiv</a>
			          <p></p>
			          <p>We use semantic scene graphs to disambiguate referring expressions in an interactive object grounding scenario. This is effectively useful in scenes with multiple identical objects.</p>
			        </td>
			      </tr>
						
          <tr onmouseout="point_stop()" onmouseover="point_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='point_image'>
                  <img src='images/point_after.png' width="160"></div>
                <img src='images/point_before.png' width="160">
              </div>
              <script type="text/javascript">
                function point_start() {
                  document.getElementById('point_image').style.opacity = "1";
                }

                function point_stop() {
                  document.getElementById('point_image').style.opacity = "0";
                }
                point_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/20/18/5029">
                <papertitle>Point Cloud Scene Completion of Obstructed Building Facades with Generative Adversarial Inpainting</papertitle>
              </a>
              <br>
              <a href="https://jingdao.github.io/">Jingdao Chen</a>,
              <strong>John Seon Keun Yi</strong>,
              <a href="https://scholar.google.com/citations?user=nLm228EAAAAJ&hl=en">Mark Kahoush</a>,
              Erin S. Cho,
              <a href="https://ce.gatech.edu/directory/person/yong-kwon-cho">Yong Kwon Cho</a>
              <br>
              <em>Sensors</em>, 2020  
              <br>
              <a href="https://www.mdpi.com/1424-8220/20/18/5029">paper</a>
              <p></p>
              <p>We use 2D inpainting methods to complete occlusions and imperfections in 3D building point cloud scans.</p>
            </td>
          </tr> 
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr onmouseout="dog_stop()" onmouseover="dog_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dog_image'>
                  <img src='images/dog_after.gif' width="160"></div>
                <img src='images/dog_before.png' height="160" width="160">
              </div>
              <script type="text/javascript">
                function dog_start() {
                  document.getElementById('dog_image').style.opacity = "1";
                }

                function dog_stop() {
                  document.getElementById('dog_image').style.opacity = "0";
                }
                dog_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Force Interaction Model for Robot Guide Dog</papertitle>
              <br>
              PI: 
              <a href="https://faculty.cc.gatech.edu/~sha9/">Sehoon Ha</a>, 
              <a href="http://sonify.psych.gatech.edu/~walkerb/">Bruce Walker</a>
              <br>
              <p></p>
              <p>
An HRI model for a guide dog robot that distinguishes different force commands from the attached harness and reacts by adjusting its movement.
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/8803_head.png" alt="8803_head" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning Generalizable Representations by Combining Pretext Tasks</papertitle>
              <br>
              CS8803 LS: Machine Learning with Limited Supervision - Class Project
              <br>
              <a href="https://www.youtube.com/watch?v=HOde50FdKD8">video</a>
              <p></p>
              <p>
We try to learn domain agnostic generelizable representations that yields good performance on multiple downstream tasks, by leveraging the power of multiple self-supervised pretext tasks. We demonstrate one of the proposed approaches which uses an ensemble of multiple pretext tasks to make final predictions in the downstream tasks.
              </p>
            </td>
          </tr>		

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

		  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bu_logo.png" width="160">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant: <br>
              Introduction to Computer Science: CS111 Summer 2025, Summer 2024 <br>
			  Introduction to Artificial Intelligence: CS440 Spring 2025 <br>
			  Software Engineering: CS411 Fall 2025
            </td>
		  </tr>	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gt-seal.png" width="160">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant: <br>
              Intro to Robotics and Perception: CS3630
              <a target="_blank" href="https://dellaert.github.io/22F-3630/">Fall 2022</a>,
              <a target="_blank" href="https://dellaert.github.io/22S-3630/">Spring 2022</a>,  
              <a target="_blank" href="https://harishravichandarblog.files.wordpress.com/2021/09/cs3630_fall2021_syllabus-v1-aug_22.pdf">Fall 2021</a>, 
              <a target="_blank" href="https://dellaert.github.io/21S-3630/">Spring 2021</a>, 
              Fall 2020, 
              <a target="_blank" href="https://dellaert.github.io/20S-3630/">Spring 2020</a>
            </td>
          </tr>
		  
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This page is adapted from this <a target="_blank" href="https://github.com/jonbarron/jonbarron_website">template</a> which is created by <a target="_blank" href="http://jonbarron.info">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
