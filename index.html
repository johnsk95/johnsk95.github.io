<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>John Seon Keun Yi</title>
  
  <meta name="author" content="John Seon Keun Yi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>		
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>John Seon Keun Yi</name>
              </p>
              <p>
                I am a Computer Science Ph.D student at <a href="https://www.bu.edu/cs/">Boston University</a>.
		I am fortunate to be advised by Prof. <a href="https://www.leedokyun.com/">Dokyun "DK" Lee</a> and <a href="https://ai.bu.edu/ksaenko.html">Kate Saenko</a>.
              </p>
              <p>
                <!-- I work on computer vision and its use in robotics. -->
                I received Masters and Bachelors degrees in Computer Science from <a href="https://www.cc.gatech.edu/">Georgia Institute of Technology</a>, where I worked on data-efficient learning and methodologies for human-robot interaction.
              </p>

              <p style="text-align:center">
                <a href="mailto:johnsk9595@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1udQRLXpFSVz9dAC0vALyhudbUZHsM1fb/view?usp=share_link">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.tw/citations?user=LtqmnjEAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/johnsk95">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JohnYi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/JohnYi-circle2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interest mainly focuses on multi-agentic systems. Particularly, I am interested in identifying and mitigating bias and misinformation in foundation models. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  



          
Here's the generated HTML for the publication entry:
```html


<tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="placeholder.png" alt="paper" width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#">
            <papertitle>Incremental Object Grounding Using Scene Graphs</papertitle>
        </a>
        <br>
        <strong>John Seon Keun Yi, Yoonwoo Kim, Sonia Chernova</strong>
        <br>
        <em>Preprint</em>, 2024
        <br>
        <a href="#">paper</a> / <a href="#">code</a> / <a href="#">project page</a>
        <p></p>
        <p>Our proposed model, Incremental Grounding using Scene Graphs (IGSG), is a disambiguation model that leverages semantic data from image scene graphs and linguistic structures from language scene graphs to ground objects based on human commands. The main contribution of our paper is the development of IGSG, which demonstrates promising results in complex real-world scenes where there are multiple identical target objects. Our approach can effectively disambiguate ambiguous or wrong referring expressions by asking relevant questions.</p>
    </td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="placeholder.png" alt="paper" width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2201.07459v3">
      <papertitle>PT4AL: Using Self-Supervised Pretext Tasks for Active Learning</papertitle>
    </a>
    <br>
    <strong>John Seon Keun Yi</strong>, Minseok Seo, Jongchan Park, Dong-Geol Choi
    <br>
    <em>Preprint</em>, 2024
    <br>
    <a href="#">paper</a> / <a href="#">code</a> / <a href="#">project page</a>
    <p></p>
    <p>Our novel approach utilizes self-supervised pretext tasks for active learning, which leverages the knowledge gained from these tasks to efficiently select informative and representative data points for annotation. By doing so, we demonstrate a strong correlation between the loss of a simple pretext task and the downstream task loss, making our method more effective in tackling the cold-start problem and improving active learning performance on imbalanced datasets.</p>
  </td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="placeholder.png" alt="paper" width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="#">
      <papertitle>A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond</papertitle>
    </a>
    <br>
    <strong>Chaoning Zhang, Chenshuang Zhang, Junha Song, John Seon Keun Yi, Kang Zhang, In So Kweon</strong>, Co-Author Name
    <br>
    <em>Preprint</em>, 2024
    <br>
    <a href="https://arxiv.org/pdf/2208.00173v1">paper</a> / <a href="#">project page</a> / <a href="#">code</a>
    <p></p>
    <p>The paper provides a comprehensive survey of Masked Autoencoders for Self-supervised Learning in Vision and Beyond, highlighting their contributions to the field and their potential impact on various applications.</p>
  </td>
</tr>
```
<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/placeholder.png" alt="Global PIQA: Evaluat" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2510.24081v1">
                <papertitle>Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</papertitle>
              </a>
              <br>
              Tyler A. Chang, Catherine Arnett, Abdelrahman Eldesokey, Abdelrahman Sadallah, Abeer Kashar, Abolade Daud, Abosede Grace Olanihun, Adamu Labaran Mohammed, Adeyemi Praise, Adhikarinayum Meerajita Sharma, Aditi Gupta, Afitab Iyigun, Afonso Simpl√≠cio, Ahmed Essouaied, Aicha Chorana, Akhil Eppa, Akintunde Oladipo, Akshay Ramesh, Aleksei Dorkin, Alfred Malengo Kondoro, Alham Fikri Aji, Ali Eren √áetinta≈ü, Allan Hanbury, Alou Dembele, Alp Niksarli, √Ålvaro Arroyo, Amin Bajand, Amol Khanna, Ana Chkhaidze, Ana Condez, Andiswa Mkhonto, Andrew Hoblitzell, Andrew Tran, Angelos Poulis, Anirban Majumder, Anna Vacalopoulou, Annette Kuuipolani Kanahele Wong, Annika Simonsen, Anton Kovalev, Ashvanth. S, Ayodeji Joseph Lana, Barkin Kinay, Bashar Alhafni, Benedict Cibalinda Busole, Bernard Ghanem, Bharti Nathani, Biljana Stojanovska ƒêuriƒá, Bola Agbonile, Bragi Bergsson, Bruce Torres Fischer, Burak Tutar, Burcu Alaku≈ü √áƒ±nar, Cade J. Kanoniakapueo Kane, Can Udomcharoenchaikit, Catherine Arnett, Chadi Helwe, Chaithra Reddy Nerella, Chen Cecilia Liu, Chiamaka Glory Nwokolo, Cristina Espa√±a-Bonet, Cynthia Amol, DaeYeop Lee, Dana Arad, Daniil Dzenhaliou, Daria Pugacheva, Dasol Choi, Daud Abolade, David Liu, David Semedo, Deborah Popoola, Deividas Mataciunas, Delphine Nyaboke, Dhyuthy Krishna Kumar, Diogo Gl√≥ria-Silva, Diogo Tavares, Divyanshu Goyal, DongGeon Lee, Ebele Nwamaka Anajemba, Egonu Ngozi Grace, Elena Mickel, Elena Tutubalina, Elias Herranen, Emile Anand, Emmanuel Habumuremyi, Emuobonuvie Maria Ajiboye, Eryawan Presma Yulianrifat, Esther Adenuga, Ewa Rudnicka, Faith Olabisi Itiola, Faran Taimoor Butt, Fathima Thekkekara, Fatima Haouari, Filbert Aurelian Tjiaranata, Firas Laakom, Francesca Grasso, Francesco Orabona, Francesco Periti, Gbenga Kayode Solomon, Gia Nghia Ngo, Gloria Udhehdhe-oze, Gon√ßalo Martins, Gopi Naga Sai Ram Challagolla, Guijin Son, Gulnaz Abdykadyrova, Hafsteinn Einarsson, Hai Hu, Hamidreza Saffari, Hamza Zaidi, Haopeng Zhang, Harethah Abu Shairah, Harry Vuong, Hele-Andra Kuulmets, Houda Bouamor, Hwanjo Yu, Iben Nyholm Debess, ƒ∞brahim Ethem Deveci, Ikhlasul Akmal Hanif, Ikhyun Cho, In√™s Calvo, In√™s Vieira, Isaac Manzi, Ismail Daud, Itay Itzhak, Iuliia, Alekseenko, Ivan Belashkin, Ivan Spada, Ivan Zhelyazkov, Jacob Brinton, Jafar Isbarov, Jaka ƒåibej, Jan ƒåuhel, Jan Koco≈Ñ, Jauza Akbar Krito, Jebish Purbey, Jennifer Mickel, Jennifer Za, Jenny Kunz, Jihae Jeong, Jimena Tena D√°valos, Jinu Lee, Jo√£o Magalh√£es, <strong>John Yi</strong>, Jongin Kim, Joseph Chataignon, Joseph Marvin Imperial, Jubeerathan Thevakumar, Judith Land, Junchen Jiang, Jungwhan Kim, Kairit Sirts, Kamesh R, Kamesh V, Kanda Patrick Tshinu, K√§triin Kukk, Kaustubh Ponkshe, Kavsar Huseynova, Ke He, Kelly Buchanan, Kengatharaiyer Sarveswaran, Kerem Zaman, Khalil Mrini, Kian Kyars, Krister Kruusmaa, Kusum Chouhan, Lainitha Krishnakumar, Laura Castro S√°nchez, Laura Porrino Moscoso, Leshem Choshen, Levent Sencan, Lilja √òvrelid, Lisa Alazraki, Lovina Ehimen-Ugbede, Luheerathan Thevakumar, Luxshan Thavarasa, Mahnoor Malik, Mamadou K. Keita, Mansi Jangid, Marco De Santis, Marcos Garc√≠a, Marek Suppa, Mariam D'Ciofalo, Marii Ojastu, Maryam Sikander, Mausami Narayan, Maximos Skandalis, Mehak Mehak, Mehmet ƒ∞lteri≈ü Bozkurt, Melaku Bayu Workie, Menan Velayuthan, Michael Leventhal, Micha≈Ç Marci≈Ñczuk, Mirna Potoƒçnjak, Mohammadamin Shafiei, Mridul Sharma, Mrityunjaya Indoria, Muhammad Ravi Shulthan Habibi, Murat Koliƒá, Nada Galant, Naphat Permpredanun, Narada Maugin, Nicholas Kluge Corr√™a, Nikola Ljube≈°iƒá, Nirmal Thomas, Nisansa de Silva, Nisheeth Joshi, Nitish Ponkshe, Nizar Habash, Nneoma C. Udeze, Noel Thomas, No√©mi Ligeti-Nagy, Nouhoum Coulibaly, Nsengiyumva Faustin, Odunayo Kareemat Buliaminu, Odunayo Ogundepo, Oghojafor Godswill Fejiro, Ogundipe Blessing Funmilola, Okechukwu God'spraise, Olanrewaju Samuel, Olaoye Deborah Oluwaseun, Olasoji Akindejoye, Olga Popova, Olga Snissarenko, Onyinye Anulika Chiemezie, Orkun Kinay, Osman Tursun, Owoeye Tobiloba Moses, Oyelade Oluwafemi Joshua, Oyesanmi Fiyinfoluwa, Pablo Gamallo, Pablo Rodr√≠guez Fern√°ndez, Palak Arora, Pedro Valente, Peter Rupnik, Philip Oghenesuowho Ekiugbo, Pramit Sahoo, Prokopis Prokopidis, Pua Niau-Puhipau, Quadri Yahya, Rachele Mignone, Raghav Singhal, Ram Mohan Rao Kadiyala, Raphael Merx, Rapheal Afolayan, Ratnavel Rajalakshmi, Rishav Ghosh, Romina Oji, Ron Kekeha Solis, Rui Guerra, Rushikesh Zawar, Sa'ad Nasir Bashir, Saeed Alzaabi, Sahil Sandeep, Sai Pavan Batchu, SaiSandeep Kantareddy, Salsabila Zahirah Pranida, Sam Buchanan, Samuel Rutunda, Sander Land, Sarah Sulollari, Sardar Ali, Saroj Sapkota, Saulius Tautvaisas, Sayambhu Sen, Sayantani Banerjee, Sebastien Diarra, SenthilNathan. M, Sewoong Lee, Shaan Shah, Shankar Venkitachalam, Sharifa Djurabaeva, Sharon Ibejih, Shivanya Shomir Dutta, Siddhant Gupta, Silvia Paniagua Su√°rez, Sina Ahmadi, Sivasuthan Sukumar, Siyuan Song, Snegha A., Sokratis Sofianopoulos, Sona Elza Simon, Sonja Benƒçina, Sophie Gvasalia, Sphurti Kirit More, Spyros Dragazis, Stephan P. Kaufhold, Suba. S, Sultan AlRashed, Surangika Ranathunga, Taiga Someya, Taja Kuzman Punger≈°ek, Tal Haklay, Tasi'u Jibril, Tatsuya Aoyama, Tea Abashidze, Terenz Jomar Dela Cruz, Terra Blevins, Themistoklis Nikas, Theresa Dora Idoko, Thu Mai Do, Tilek Chubakov, Tommaso Gargiani, Uma Rathore, Uni Johannesen, Uwuma Doris Ugwu, Vallerie Alexandra Putra, Vanya Bannihatti Kumar, Varsha Jeyarajalingam, Varvara Arzt, Vasudevan Nedumpozhimana, Viktoria Ondrejova, Viktoryia Horbik, Vishnu Vardhan Reddy Kummitha, Vuk Diniƒá, Walelign Tewabe Sewunetie, Winston Wu, Xiaojing Zhao, Yacouba Diarra, Yaniv Nikankin, Yash Mathur, <strong>Yixi Chen</strong>, <strong>Yiyuan Li</strong>, Yolanda Xavier, Yonatan Belinkov, Yusuf Ismail Abayomi, Zaid Alyafeai, Zhengyang Shan, Zhi Rui Tam, Zilu Tang, Zuzana Nadova, Baber Abbasi, Stella Biderman, David Stap, Duygu Ataman, Fabian Schmidt, Hila Gonen, Jiayi Wang, David Ifeoluwa Adelani
              <br>
              <em>Preprint</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2510.24081v1">paper</a>
              <p></p>
              <p>The paper presents the creation of Global PIQA, an evaluation benchmark for large language models (LLMs) that covers 100+ languages and cultures. The dataset consists of over 50% examples that reference local foods, customs, traditions, or other culturally-specific elements, which highlights the importance of considering cultural differences in LLM performance evaluation. The paper finds that state-of-the-art LLMs perform well on Global</p>
            </td>
          </tr>

<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="placeholder.png" alt="paper" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2510.24081v1">
                <papertitle>Global piqa: Evaluating physical commonsense reasoning across 100+ languages and cultures</papertitle>
              </a>
              <br>
              <strong>Tyler A. Chang, Catherine Arnett, Abdelrahman Eldesokey, Abdelrahman Sadallah, Abeer Kashar, Abolade Daud, Abosede Grace Olanihun, Adamu Labaran Mohammed, Adeyimi Praise, Aditi Gupta, Afitab Iyigun, Afonso Simpl√≠cio, Ahmed Essouaied, Aicha Chorana, Akhil Eppa, Akintunde Oladipo, Akshay Ramesh, Aleksei Dorkin, Alfred Malengo Kondoro, Alham Fikri Aji, Ali Eren √áetinta≈ü, Allan Hanbury, Alou Dembele, Alp Niksarli, √Ålvaro Arroyo, Amin Bajand, Amol Khanna, Ana Chkhaidze, Ana Condez, Andiswa Mkhonto, Andrew Hoblitzell, Andrew Tran, Angelos Poulis, Anirban Majumder, Anna Vacalopoulou, Annette Kuuipolani Kanahele Wong, Annika Simonsen, Anton Kovalev, Ashvanth. S, Ayodeji Joseph Lana, Barkin Kinay, Bashar Alhafni, Benedict Cibalinda Busole, Bernard Ghanem, Bharti Nathani, Biljana Stojanovska ƒêuriƒá, Bola Agbonile, Bragi Bergsson, Bruce Torres Fischer, Burak Tutar, Burcu Alaku
<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="placeholder.png" alt="paper" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#">
                <papertitle>A Survey on Masked Autoencoder for Visual Self-supervised Learning.</papertitle>
              </a>
              <br>
              <strong>Venue</strong>, 2023
              <br>
              <em>Conference Name</em>
              <br>
              <a href="#">project page</a> / <a href="#">code</a> / <a href="#">paper</a>
              <p></p>
              <p>Brief description of the paper contribution.</p>
            </td>
          </tr>
<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infdriver.png" alt="8803_head" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://infdriver.github.io/">
                <papertitle>Uncertainty-Guided Never-Ending Learning to Drive</papertitle>
              </a>
              <br>
	      <a href="https://leilai125.github.io/">Lei Lai</a>, 
              <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>, 
              <a href="https://research.redhat.com/blog/project_member/sanjay-arora/">Sanjay Arora</a>,
              <strong>John Seon Keun Yi</strong>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://infdriver.github.io/">project page</a> /
              <a href="https://github.com/h2xlab/InfDriver">code</a> / 
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_Uncertainty-Guided_Never-Ending_Learning_to_Drive_CVPR_2024_paper.pdf">paper</a>
              <p></p>
              <p>
Continuously learning to drive from an infinite flow of YouTube driving videos.
              </p>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mae_flow.jpg" alt="mae_flow" width="160" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://www.ijcai.org/proceedings/2023/0762.pdf">
                <papertitle>A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond</papertitle>
              </a>
              <br>
              <a href="https://chaoningzhang.github.io/">Chaoning Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=HbqjLHYAAAAJ">Chenshuang Zhang,</a>, 
              <a href="https://junha1125.github.io/about/">Junha Song</a>, 
              <strong>John Seon Keun Yi</strong>,
              <a href="https://scholar.google.com/citations?user=XA8EOlEAAAAJ">In So Kweon</a>
              <br>
              <em>IJCAI</em>, 2023  
              <br>
							<a href="https://arxiv.org/pdf/2208.00173">arXiv</a>
              <p></p>
              <p>A comprehensive survey on Masked Autoencoders(MAE) in vision and other fields.</p>
            </td>
          </tr>
		
          <tr onmouseout="pt4al_stop()" onmouseover="pt4al_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pt4al_image'>
                  <img src='images/pt4al_after.jpeg' width="160"></div>
                <img src='images/pt4al_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function pt4al_start() {
                  document.getElementById('pt4al_image').style.opacity = "1";
                }

                function pt4al_stop() {
                  document.getElementById('pt4al_image').style.opacity = "0";
                }
                pt4al_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://seominseok0429.github.io/PT4AL/">
                <papertitle>PT4AL: Using Self-Supervised Pretext Tasks for Active Learning</papertitle>
              </a>
              <br>
              <strong>John Seon Keun Yi*</strong>,
              <a href="https://sites.google.com/view/minseokcv/">Minseok Seo*</a>, 
              <a href="https://sites.google.com/view/jongchanpark">Jongchan Park</a>, 
              <a href="https://sites.google.com/site/dgchoicv/">Dong-Geol Choi</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://seominseok0429.github.io/PT4AL/">project page</a> /
              <a href="https://www.youtube.com/watch?v=SN7U13MMbOk">video</a> /
              <a href="https://github.com/johnsk95/PT4AL">code</a> / 
              <a href="https://arxiv.org/abs/2201.07459">arXiv</a>
              <p></p>
              <p>
We use simple self-supervised pretext tasks and a loss-based sampler to sample both representative and difficult data from an unlabeled pool.
              </p>
            </td>
          </tr>		


			    <tr onmouseout="scene_stop()" onmouseover="scene_start()">
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
                <div class="two" id='scene_image'>
                  <img src='images/scene_before.png' height="170" width="160"></div>
                <img src='images/scene_after.png' height="170" width="160">
              </div>
			        <script type="text/javascript">
			          function scene_start() {
			            document.getElementById('scene_image').style.opacity = "1";
			          }

			          function scene_stop() {
			            document.getElementById('scene_image').style.opacity = "0";
			          }
			          scene_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			          <a href="https://arxiv.org/pdf/2201.01901.pdf">
			            <papertitle>Incremental Object Grounding Using Scene Graphs</papertitle>
			          </a>
			          <br>
			          <strong>John Seon Keun Yi*</strong>,
			          <a href="https://github.com/ywkim0606">Yoonwoo Kim*</a>,
			          <a href="https://faculty.cc.gatech.edu/~chernova/">Sonia Chernova</a>
                <br>
                <em>arXiv</em>, 2021  
                <br>
			          <a href="https://arxiv.org/pdf/2201.01901">arXiv</a>
			          <p></p>
			          <p>We use semantic scene graphs to disambiguate referring expressions in an interactive object grounding scenario. This is effectively useful in scenes with multiple identical objects.</p>
			        </td>
			      </tr>
						
          <tr onmouseout="point_stop()" onmouseover="point_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='point_image'>
                  <img src='images/point_after.png' width="160"></div>
                <img src='images/point_before.png' width="160">
              </div>
              <script type="text/javascript">
                function point_start() {
                  document.getElementById('point_image').style.opacity = "1";
                }

                function point_stop() {
                  document.getElementById('point_image').style.opacity = "0";
                }
                point_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/20/18/5029">
                <papertitle>Point Cloud Scene Completion of Obstructed Building Facades with Generative Adversarial Inpainting</papertitle>
              </a>
              <br>
              <a href="https://jingdao.github.io/">Jingdao Chen</a>,
              <strong>John Seon Keun Yi</strong>,
              <a href="https://scholar.google.com/citations?user=nLm228EAAAAJ&hl=en">Mark Kahoush</a>,
              Erin S. Cho,
              <a href="https://ce.gatech.edu/directory/person/yong-kwon-cho">Yong Kwon Cho</a>
              <br>
              <em>Sensors</em>, 2020  
              <br>
              <a href="https://www.mdpi.com/1424-8220/20/18/5029">paper</a>
              <p></p>
              <p>We use 2D inpainting methods to complete occlusions and imperfections in 3D building point cloud scans.</p>
            </td>
          </tr> 
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr onmouseout="dog_stop()" onmouseover="dog_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dog_image'>
                  <img src='images/dog_after.gif' width="160"></div>
                <img src='images/dog_before.png' height="160" width="160">
              </div>
              <script type="text/javascript">
                function dog_start() {
                  document.getElementById('dog_image').style.opacity = "1";
                }

                function dog_stop() {
                  document.getElementById('dog_image').style.opacity = "0";
                }
                dog_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Force Interaction Model for Robot Guide Dog</papertitle>
              <br>
              PI: 
              <a href="https://faculty.cc.gatech.edu/~sha9/">Sehoon Ha</a>, 
              <a href="http://sonify.psych.gatech.edu/~walkerb/">Bruce Walker</a>
              <br>
              <p></p>
              <p>
An HRI model for a guide dog robot that distinguishes different force commands from the attached harness and reacts by adjusting its movement.
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/8803_head.png" alt="8803_head" width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning Generalizable Representations by Combining Pretext Tasks</papertitle>
              <br>
              CS8803 LS: Machine Learning with Limited Supervision - Class Project
              <br>
              <a href="https://www.youtube.com/watch?v=HOde50FdKD8">video</a>
              <p></p>
              <p>
We try to learn domain agnostic generelizable representations that yields good performance on multiple downstream tasks, by leveraging the power of multiple self-supervised pretext tasks. We demonstrate one of the proposed approaches which uses an ensemble of multiple pretext tasks to make final predictions in the downstream tasks.
              </p>
            </td>
          </tr>		

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

		  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bu_logo.png" width="160">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant: <br>
              Introduction to Computer Science: CS111 Summer 2025, Summer 2024 <br>
			  Introduction to Artificial Intelligence: CS440 Spring 2025 <br>
			  Software Engineering: CS411 Fall 2025
            </td>
		  </tr>	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gt-seal.png" width="160">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant: <br>
              Intro to Robotics and Perception: CS3630
              <a target="_blank" href="https://dellaert.github.io/22F-3630/">Fall 2022</a>,
              <a target="_blank" href="https://dellaert.github.io/22S-3630/">Spring 2022</a>,  
              <a target="_blank" href="https://harishravichandarblog.files.wordpress.com/2021/09/cs3630_fall2021_syllabus-v1-aug_22.pdf">Fall 2021</a>, 
              <a target="_blank" href="https://dellaert.github.io/21S-3630/">Spring 2021</a>, 
              Fall 2020, 
              <a target="_blank" href="https://dellaert.github.io/20S-3630/">Spring 2020</a>
            </td>
          </tr>
		  
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This page is adapted from this <a target="_blank" href="https://github.com/jonbarron/jonbarron_website">template</a> which is created by <a target="_blank" href="http://jonbarron.info">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
